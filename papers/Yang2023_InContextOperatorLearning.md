---
- date: 2023-09-26
- url: http://arxiv.org/abs/2304.07993
- DOI: 10.1073/pnas.2310142120
- citekey: Yang2023_InContextOperatorLearning
---

# In-Context Operator Learning with Data Prompts for Differential Equation Problems

#### Liu Yang, Siting Liu, Tingwei Meng, Stanley J. Osher

### Abstract

This paper introduces a new neural-network-based approach, namely In-Context Operator Networks (ICON), to simultaneously learn operators from the prompted data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos in the prompt are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ordinary differential equations (ODEs), partial differential equations (PDEs), and mean-field control (MFC) problems, and also show that it can generalize its learning capability to operators beyond the training distribution.

---

## Ojective

Implement **in-context learning** for solving PDE regardless of the system or its parameters.

## Problem
<!-- regression / classification / génération ? -->
<!-- finetuning / adaptive learning ? -->
<!-- parametric / multiphysics ? -->

Formulates a **regression** problem.

Manages to do **parametric** and even hints of **multiphysics** through **in-context learning**.

## Methodology
<!-- accent on encoding -->
<!-- transformer ? -->

#### Input

The **evaluation at various points** of different "Condition" functions and their related "Quantities of Interest" (QoI) functions (in our case the Condition is the **past evolution** and the QoI is the **future evolution**). The domain is **1D**.

#### Encoding

Tokens are generated **for every point** of every Condition/QoI pair. The information of the coordinates (here only $t$ and $x$), of the value of the function at this position $u(t, x)$ and the index of the example (an integer $i$ referring to which pair this point is associated to, $i$ for the Condition and $-i$ for the QoI) are concatenated in a token.

Tokens are generated for all the Condition/QoI pairs that constitutes **the context** (examples that follow the same dynamic) as well as the Condition of the system to predict (the **Question Condition**).

This token sequence is fed to a **Transformer Encoder** that encodes them in an Operator and Question embedding.

#### Decoder

The decoder is implemented as a **cross-attention Transformer**, which takes the Operator and Question embedding previously encoded as $K, V$ and the tokens generated by **the coordinates of the points to evaluate the Question QoI** as $Q$. It decodes the values $u(t, x)$ of the question function at those coordinates. 

## Experiments

### Data

**19 types of problems** were generated for training, each of which has 1000 sets of parameters ; a 100 condition-QoI pairs were generated for each operator :
- forward and inverse versions of :
	- 3 ODEs
	- Damped oscillator
	- Poisson equation
	- Linear reaction-diffusion
	- Nonlinear reaction-diffusion
- 5 Mean-Field Control systems

### Results

This model demonstrates accurate prediction capabilities even with systems parameters extending **beyond the training domain**.

**Preliminary** evidence of the model's ability to learn and apply operators for **unseen equations** was also observed.

## Limitations

The training and testing data is restricted to **only ODEs** and **only 1D**.

The encoding of each and every point as a token generates a **very long sequence** for a Transformer.

The feeding of past/future pairs to the Transformer Encoder makes it **redundant** in the case where these pairs correspond to successive chunks of a single example trajectory.
# Litterature
Literature resources for spatiotemp

## Foundation Models

- [BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid Dynamics](/papers/Liu2025_BCATBlockCausal.md) - [arxiv.org/abs/2501.18972](http://arxiv.org/abs/2501.18972)
- [Masked Autoencoders are PDE Learners](/papers/Zhou2024_MaskedAutoencodersAre.md) - [arxiv.org/abs/2403.17728](http://arxiv.org/abs/2403.17728)
- [Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs](/papers/Rahman2024_PretrainingCodomainAttention.md) - [arxiv.org/abs/2403.12553](http://arxiv.org/abs/2403.12553)
- [In-Context Operator Learning with Data Prompts for Differential Equation Problems](/papers/Yang2023_InContextOperatorLearning.md) - [arxiv.org/abs/2304.07993](http://arxiv.org/abs/2304.07993)
- [Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation](/papers/Sun2025_FoundationModelPartial.md) - [arxiv.org/abs/2404.12355](http://arxiv.org/abs/2404.12355)
- [Poseidon: Efficient Foundation Models for PDEs](/papers/Herde2024_PoseidonEfficientFoundation.md) - [arxiv.org/abs/2405.19101](http://arxiv.org/abs/2405.19101)
- [Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators](/papers/Alkin2024_UniversalPhysicsTransformers.md) - [arxiv.org/abs/2402.12365](http://arxiv.org/abs/2402.12365)
- [UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation](/papers/Shen2024_UPSEfficientlyBuilding.md) - [arxiv.org/abs/2403.07187](http://arxiv.org/abs/2403.07187)
- [DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training](/papers/Hao2024_DPOTAutoRegressiveDenoising.md) - [arxiv.org/abs/2403.03542v4](https://arxiv.org/abs/2403.03542v4)
- [Multiple Physics Pretraining for Physical Surrogate Models](/papers/McCabe2023_MultiplePhysicsPretraining.md) - [arxiv.org/abs/2310.02994](http://arxiv.org/abs/2310.02994)
- [Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior](/papers/Subramanian2023_FoundationModelsScientific.md) - [arxiv.org/abs/2306.00258](http://arxiv.org/abs/2306.00258)

â†’ [Synthesis](/notes/foundation_models.md)

## Team

- [GEPS: Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning](/papers/Koupai2024_GEPSBoostingGeneralization.md)
- [Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs](/papers/Serrano2024_ZebraInContextGenerative.md)
- [AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields](/papers/Serrano2024_AROMAPreservingSpatial.md)

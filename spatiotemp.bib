@online{Alkin2024_UniversalPhysicsTransformers,
  title = {Universal {{Physics Transformers}}: {{A Framework For Efficiently Scaling Neural Operators}}},
  shorttitle = {Universal {{Physics Transformers}}},
  author = {Alkin, Benedikt and Fürst, Andreas and Schmid, Simon and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes},
  date = {2024-10-10},
  eprint = {2402.12365},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.12365},
  url = {http://arxiv.org/abs/2402.12365},
  urldate = {2024-11-12},
  abstract = {Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.},
  pubstate = {prepublished}
}

@online{Hao2024_DPOTAutoRegressiveDenoising,
  title = {{{DPOT}}: {{Auto-Regressive Denoising Operator Transformer}} for {{Large-Scale PDE Pre-Training}}},
  shorttitle = {{{DPOT}}},
  author = {Hao, Zhongkai and Su, Chang and Liu, Songming and Berner, Julius and Ying, Chengyang and Su, Hang and Anandkumar, Anima and Song, Jian and Zhu, Jun},
  date = {2024-03-06},
  url = {https://arxiv.org/abs/2403.03542v4},
  urldate = {2024-06-21},
  abstract = {Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \textbackslash url\{https://github.com/thu-ml/DPOT\}.},
  langid = {english},
  organization = {arXiv.org}
}

@online{Herde2024_PoseidonEfficientFoundation,
  title = {Poseidon: {{Efficient Foundation Models}} for {{PDEs}}},
  shorttitle = {Poseidon},
  author = {Herde, Maximilian and Raonić, Bogdan and Rohner, Tobias and Käppeli, Roger and Molinaro, Roberto and family=Bézenac, given=Emmanuel, prefix=de, useprefix=false and Mishra, Siddhartha},
  date = {2024-11-05},
  eprint = {2405.19101},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.19101},
  url = {http://arxiv.org/abs/2405.19101},
  urldate = {2024-11-12},
  abstract = {We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.},
  pubstate = {prepublished}
}

@online{Koupai2024_GEPSBoostingGeneralization,
  title = {{{GEPS}}: {{Boosting Generalization}} in {{Parametric PDE Neural Solvers}} through {{Adaptive Conditioning}}},
  shorttitle = {{{GEPS}}},
  author = {Koupaï, Armand Kassaï and Benet, Jorge Mifsut and Yin, Yuan and Vittaut, Jean-Noël and Gallinari, Patrick},
  date = {2024-11-08},
  eprint = {2410.23889},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.23889},
  url = {http://arxiv.org/abs/2410.23889},
  urldate = {2024-11-12},
  abstract = {Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters. Machine learning approaches often struggle to capture this variability. To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters. We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, \$\textbackslash textit\{adaptive conditioning\}\$, allows stronger generalization. As existing adaptive conditioning methods do not scale well with respect to the number of parameters to adapt in the neural solver, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters. We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers. Validation performed on a whole range of spatio-temporal forecasting problems demonstrates excellent performance for generalizing to unseen conditions including initial conditions, PDE coefficients, forcing terms and solution domain. \$\textbackslash textit\{Project page\}\$: https://geps-project.github.io},
  pubstate = {prepublished}
}

@online{Li2023_TransformerPartialDifferential,
  title = {Transformer for {{Partial Differential Equations}}' {{Operator Learning}}},
  author = {Li, Zijie and Meidani, Kazem and Farimani, Amir Barati},
  date = {2023-04-27},
  eprint = {2205.13671},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2205.13671},
  url = {http://arxiv.org/abs/2205.13671},
  urldate = {2024-11-28},
  abstract = {Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed framework is competitive on standard benchmark problems and can flexibly be adapted to randomly sampled input.},
  pubstate = {prepublished}
}

@online{McCabe2023_MultiplePhysicsPretraining,
  title = {Multiple {{Physics Pretraining}} for {{Physical Surrogate Models}}},
  author = {McCabe, Michael and Blancard, Bruno Régaldo-Saint and Parker, Liam Holden and Ohana, Ruben and Cranmer, Miles and Bietti, Alberto and Eickenberg, Michael and Golkar, Siavash and Krawezik, Geraud and Lanusse, Francois and Pettee, Mariel and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  date = {2023-10-04},
  eprint = {2310.02994},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2310.02994},
  url = {http://arxiv.org/abs/2310.02994},
  urldate = {2024-11-12},
  abstract = {We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from scratch or finetuning pretrained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility and community experimentation.},
  pubstate = {prepublished}
}

@online{Serrano2024_AROMAPreservingSpatial,
  title = {{{AROMA}}: {{Preserving Spatial Structure}} for {{Latent PDE Modeling}} with {{Local Neural Fields}}},
  shorttitle = {{{AROMA}}},
  author = {Serrano, Louis and Wang, Thomas X. and Naour, Etienne Le and Vittaut, Jean-Noël and Gallinari, Patrick},
  date = {2024-10-21},
  eprint = {2406.02176},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2406.02176},
  url = {http://arxiv.org/abs/2406.02176},
  urldate = {2024-11-12},
  abstract = {We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.},
  pubstate = {prepublished}
}

@online{Serrano2024_ZebraInContextGenerative,
  title = {Zebra: {{In-Context}} and {{Generative Pretraining}} for {{Solving Parametric PDEs}}},
  shorttitle = {Zebra},
  author = {Serrano, Louis and Koupaï, Armand Kassaï and Wang, Thomas X. and Erbacher, Pierre and Gallinari, Patrick},
  date = {2024-10-08},
  eprint = {2410.03437},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.03437},
  url = {http://arxiv.org/abs/2410.03437},
  urldate = {2025-01-13},
  abstract = {Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.},
  pubstate = {prepublished}
}

@online{Shen2024_UPSEfficientlyBuilding,
  title = {{{UPS}}: {{Efficiently Building Foundation Models}} for {{PDE Solving}} via {{Cross-Modal Adaptation}}},
  shorttitle = {{{UPS}}},
  author = {Shen, Junhong and Marwah, Tanya and Talwalkar, Ameet},
  date = {2024-07-31},
  eprint = {2403.07187},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.07187},
  url = {http://arxiv.org/abs/2403.07187},
  urldate = {2024-11-12},
  abstract = {We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developing unified neural operators for diverse families of spatiotemporal PDEs from various domains, dimensions, and resolutions. UPS embeds different PDEs into a shared representation space and processes them using a FNO-transformer architecture. Rather than training the network from scratch, which is data-demanding and computationally expensive, we warm-start the transformer from pretrained LLMs and perform explicit alignment to reduce the modality gap while improving data and compute efficiency. The cross-modal UPS achieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench, outperforming existing unified models using 4 times less data and 26 times less compute. Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.},
  pubstate = {prepublished}
}

@online{Subramanian2023_FoundationModelsScientific,
  title = {Towards {{Foundation Models}} for {{Scientific Machine Learning}}: {{Characterizing Scaling}} and {{Transfer Behavior}}},
  shorttitle = {Towards {{Foundation Models}} for {{Scientific Machine Learning}}},
  author = {Subramanian, Shashank and Harrington, Peter and Keutzer, Kurt and Bhimji, Wahid and Morozov, Dmitriy and Mahoney, Michael and Gholami, Amir},
  date = {2023-06-01},
  eprint = {2306.00258},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2306.00258},
  url = {http://arxiv.org/abs/2306.00258},
  urldate = {2024-11-12},
  abstract = {Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the "pre-train and fine-tune" paradigm for SciML problems, demonstrating a path towards building SciML foundation models. We open-source our code for reproducibility.},
  pubstate = {prepublished}
}

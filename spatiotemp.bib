@misc{Alkin2024_UniversalPhysicsTransformers,
  title = {Universal {{Physics Transformers}}: {{A Framework For Efficiently Scaling Neural Operators}}},
  shorttitle = {Universal {{Physics Transformers}}},
  author = {Alkin, Benedikt and F{\"u}rst, Andreas and Schmid, Simon and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes},
  year = {2024},
  month = oct,
  number = {arXiv:2402.12365},
  eprint = {2402.12365},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.12365},
  url = {http://arxiv.org/abs/2402.12365},
  abstract = {Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.},
  archiveprefix = {arXiv}
}

@misc{Hao2024_DPOTAutoRegressiveDenoising,
  title = {{{DPOT}}: {{Auto-Regressive Denoising Operator Transformer}} for {{Large-Scale PDE Pre-Training}}},
  shorttitle = {{{DPOT}}},
  author = {Hao, Zhongkai and Su, Chang and Liu, Songming and Berner, Julius and Ying, Chengyang and Su, Hang and Anandkumar, Anima and Song, Jian and Zhu, Jun},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2403.03542v4},
  abstract = {Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at {\textbackslash}url\{https://github.com/thu-ml/DPOT\}.},
  langid = {english}
}

@misc{Herde2024_PoseidonEfficientFoundation,
  title = {Poseidon: {{Efficient Foundation Models}} for {{PDEs}}},
  shorttitle = {Poseidon},
  author = {Herde, Maximilian and Raoni{\'c}, Bogdan and Rohner, Tobias and K{\"a}ppeli, Roger and Molinaro, Roberto and de B{\'e}zenac, Emmanuel and Mishra, Siddhartha},
  year = {2024},
  month = nov,
  number = {arXiv:2405.19101},
  eprint = {2405.19101},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19101},
  url = {http://arxiv.org/abs/2405.19101},
  abstract = {We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.},
  archiveprefix = {arXiv}
}

@misc{Koupai2024_GEPSBoostingGeneralization,
  title = {{{GEPS}}: {{Boosting Generalization}} in {{Parametric PDE Neural Solvers}} through {{Adaptive Conditioning}}},
  shorttitle = {{{GEPS}}},
  author = {Koupa{\"i}, Armand Kassa{\"i} and Benet, Jorge Mifsut and Yin, Yuan and Vittaut, Jean-No{\"e}l and Gallinari, Patrick},
  year = {2024},
  month = nov,
  number = {arXiv:2410.23889},
  eprint = {2410.23889},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.23889},
  url = {http://arxiv.org/abs/2410.23889},
  abstract = {Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters. Machine learning approaches often struggle to capture this variability. To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters. We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, \${\textbackslash}textit\{adaptive conditioning\}\$, allows stronger generalization. As existing adaptive conditioning methods do not scale well with respect to the number of parameters to adapt in the neural solver, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters. We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers. Validation performed on a whole range of spatio-temporal forecasting problems demonstrates excellent performance for generalizing to unseen conditions including initial conditions, PDE coefficients, forcing terms and solution domain. \${\textbackslash}textit\{Project page\}\$: https://geps-project.github.io},
  archiveprefix = {arXiv}
}

@misc{Liu2025_BCATBlockCausal,
  title = {{{BCAT}}: {{A Block Causal Transformer}} for {{PDE Foundation Models}} for {{Fluid Dynamics}}},
  shorttitle = {{{BCAT}}},
  author = {Liu, Yuxuan and Sun, Jingmin and Schaeffer, Hayden},
  year = {2025},
  month = jan,
  number = {arXiv:2501.18972},
  eprint = {2501.18972},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.18972},
  url = {http://arxiv.org/abs/2501.18972},
  abstract = {We introduce BCAT, a PDE foundation model designed for autoregressive prediction of solutions to two dimensional fluid dynamics problems. Our approach uses a block causal transformer architecture to model next frame predictions, leveraging previous frames as contextual priors rather than relying solely on sub-frames or pixel-based inputs commonly used in image generation methods. This block causal framework more effectively captures the spatial dependencies inherent in nonlinear spatiotemporal dynamics and physical phenomena. In an ablation study, next frame prediction demonstrated a 2.9x accuracy improvement over next token prediction. BCAT is trained on a diverse range of fluid dynamics datasets, including incompressible and compressible Navier-Stokes equations across various geometries and parameter regimes, as well as the shallow-water equations. The model's performance was evaluated on 6 distinct downstream prediction tasks and tested on about 8K trajectories to measure robustness on a variety of fluid dynamics simulations. BCAT achieved an average relative error of 1.92\% across all evaluation tasks, outperforming prior approaches on standard benchmarks.},
  archiveprefix = {arXiv}
}

@misc{McCabe2023_MultiplePhysicsPretraining,
  title = {Multiple {{Physics Pretraining}} for {{Physical Surrogate Models}}},
  author = {McCabe, Michael and Blancard, Bruno R{\'e}galdo-Saint and Parker, Liam Holden and Ohana, Ruben and Cranmer, Miles and Bietti, Alberto and Eickenberg, Michael and Golkar, Siavash and Krawezik, Geraud and Lanusse, Francois and Pettee, Mariel and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02994},
  eprint = {2310.02994},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02994},
  url = {http://arxiv.org/abs/2310.02994},
  abstract = {We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from scratch or finetuning pretrained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility and community experimentation.},
  archiveprefix = {arXiv}
}

@misc{Rahman2024_PretrainingCodomainAttention,
  title = {Pretraining {{Codomain Attention Neural Operators}} for {{Solving Multiphysics PDEs}}},
  author = {Rahman, Md Ashiqur and George, Robert Joseph and Elleithy, Mogab and Leibovici, Daniel and Li, Zongyi and Bonev, Boris and White, Colin and Berner, Julius and Yeh, Raymond A. and Kossaifi, Jean and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  year = {2024},
  month = nov,
  number = {arXiv:2403.12553},
  eprint = {2403.12553},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.12553},
  url = {http://arxiv.org/abs/2403.12553},
  abstract = {Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-B{\textbackslash}'enard convection, we found CoDA-NO to outperform existing methods by over 36\%.},
  archiveprefix = {arXiv}
}

@misc{Serrano2024_AROMAPreservingSpatial,
  title = {{{AROMA}}: {{Preserving Spatial Structure}} for {{Latent PDE Modeling}} with {{Local Neural Fields}}},
  shorttitle = {{{AROMA}}},
  author = {Serrano, Louis and Wang, Thomas X. and Naour, Etienne Le and Vittaut, Jean-No{\"e}l and Gallinari, Patrick},
  year = {2024},
  month = oct,
  number = {arXiv:2406.02176},
  eprint = {2406.02176},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.02176},
  url = {http://arxiv.org/abs/2406.02176},
  abstract = {We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.},
  archiveprefix = {arXiv}
}

@misc{Serrano2024_ZebraInContextGenerative,
  title = {Zebra: {{In-Context}} and {{Generative Pretraining}} for {{Solving Parametric PDEs}}},
  shorttitle = {Zebra},
  author = {Serrano, Louis and Koupa{\"i}, Armand Kassa{\"i} and Wang, Thomas X. and Erbacher, Pierre and Gallinari, Patrick},
  year = {2024},
  month = oct,
  number = {arXiv:2410.03437},
  eprint = {2410.03437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03437},
  url = {http://arxiv.org/abs/2410.03437},
  abstract = {Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.},
  archiveprefix = {arXiv}
}

@misc{Shen2024_UPSEfficientlyBuilding,
  title = {{{UPS}}: {{Efficiently Building Foundation Models}} for {{PDE Solving}} via {{Cross-Modal Adaptation}}},
  shorttitle = {{{UPS}}},
  author = {Shen, Junhong and Marwah, Tanya and Talwalkar, Ameet},
  year = {2024},
  month = jul,
  number = {arXiv:2403.07187},
  eprint = {2403.07187},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07187},
  url = {http://arxiv.org/abs/2403.07187},
  abstract = {We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developing unified neural operators for diverse families of spatiotemporal PDEs from various domains, dimensions, and resolutions. UPS embeds different PDEs into a shared representation space and processes them using a FNO-transformer architecture. Rather than training the network from scratch, which is data-demanding and computationally expensive, we warm-start the transformer from pretrained LLMs and perform explicit alignment to reduce the modality gap while improving data and compute efficiency. The cross-modal UPS achieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench, outperforming existing unified models using 4 times less data and 26 times less compute. Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.},
  archiveprefix = {arXiv}
}

@misc{Subramanian2023_FoundationModelsScientific,
  title = {Towards {{Foundation Models}} for {{Scientific Machine Learning}}: {{Characterizing Scaling}} and {{Transfer Behavior}}},
  shorttitle = {Towards {{Foundation Models}} for {{Scientific Machine Learning}}},
  author = {Subramanian, Shashank and Harrington, Peter and Keutzer, Kurt and Bhimji, Wahid and Morozov, Dmitriy and Mahoney, Michael and Gholami, Amir},
  year = {2023},
  month = jun,
  number = {arXiv:2306.00258},
  eprint = {2306.00258},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.00258},
  url = {http://arxiv.org/abs/2306.00258},
  abstract = {Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the "pre-train and fine-tune" paradigm for SciML problems, demonstrating a path towards building SciML foundation models. We open-source our code for reproducibility.},
  archiveprefix = {arXiv}
}

@misc{Sun2025_FoundationModelPartial,
  title = {Towards a {{Foundation Model}} for {{Partial Differential Equations}}: {{Multi-Operator Learning}} and {{Extrapolation}}},
  shorttitle = {Towards a {{Foundation Model}} for {{Partial Differential Equations}}},
  author = {Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  year = {2025},
  month = jan,
  number = {arXiv:2404.12355},
  eprint = {2404.12355},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.12355},
  url = {http://arxiv.org/abs/2404.12355},
  abstract = {Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.},
  archiveprefix = {arXiv}
}

@article{Yang2023_InContextOperatorLearning,
  title = {In-{{Context Operator Learning}} with {{Data Prompts}} for {{Differential Equation Problems}}},
  author = {Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J.},
  year = {2023},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {39},
  eprint = {2304.07993},
  primaryclass = {cs},
  pages = {e2310142120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2310142120},
  url = {http://arxiv.org/abs/2304.07993},
  abstract = {This paper introduces a new neural-network-based approach, namely In-Context Operator Networks (ICON), to simultaneously learn operators from the prompted data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos in the prompt are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ordinary differential equations (ODEs), partial differential equations (PDEs), and mean-field control (MFC) problems, and also show that it can generalize its learning capability to operators beyond the training distribution.},
  archiveprefix = {arXiv}
}

@misc{Zhou2024_MaskedAutoencodersAre,
  title = {Masked {{Autoencoders}} Are {{PDE Learners}}},
  author = {Zhou, Anthony and Farimani, Amir Barati},
  year = {2024},
  month = dec,
  number = {arXiv:2403.17728},
  eprint = {2403.17728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.17728},
  url = {http://arxiv.org/abs/2403.17728},
  abstract = {Neural solvers for partial differential equations (PDEs) have great potential to generate fast and accurate physics solutions, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs which may encompass different coefficients, boundary conditions, resolutions, or even equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for physics problems. Through self-supervised learning across PDEs, masked autoencoders can consolidate heterogeneous physics to learn rich latent representations. We show that learned representations can generalize to a limited set of unseen equations or parameters and are meaningful enough to regress PDE coefficients or the classify PDE features. Furthermore, conditioning neural solvers on learned latent representations can improve time-stepping and super-resolution performance across a variety of coefficients, discretizations, or boundary conditions, as well as on certain unseen PDEs. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.},
  archiveprefix = {arXiv}
}
